# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O0hhDFX7ovMvCMgXHpww1kdkq6j3xvum
"""

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#load dataset
titanic_dataset = pd.read_csv('/content/Titanic-Dataset.csv')
print(titanic_dataset.describe())
print(titanic_dataset.info())
print(titanic_dataset.head())
print(titanic_dataset.tail())

numeric_columns = titanic_dataset.select_dtypes(include=["int64", "float64"])

#plot the correlations
import seaborn as sns
sns.heatmap(numeric_columns.corr(), cmap = "YlGnBu")
plt.show()

survived_counts = titanic_dataset['Survived'].value_counts()

# Plot the histogram
plt.figure(figsize=(8, 6))
plt.bar(survived_counts.index, survived_counts.values, color=['red', 'green'])
plt.title('Survival Histogram')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.xticks(survived_counts.index, ['Not Survived', 'Survived'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Filter data for survived passengers
survived = titanic_dataset[titanic_dataset['Survived'] == 1]

# Group by 'Sex' and count the number of survivors
survivors_by_sex = survived.groupby('Sex').size()

# Plotting
plt.bar(survivors_by_sex.index, survivors_by_sex.values, color=['pink', 'blue'])
plt.title('Number of Survivors by Gender')
plt.xlabel('Gender')
plt.ylabel('Number of Survivors')
plt.show()

# Group by 'Pclass' and calculate survival rate
survival_rate_by_class = titanic_dataset.groupby('Pclass')['Survived'].mean()

# Plotting
plt.bar(survival_rate_by_class.index, survival_rate_by_class.values, color=['red', 'blue', 'green'])
plt.title('Survival Rate by Passenger Class')
plt.xlabel('Passenger Class')
plt.ylabel('Survival Rate')
plt.xticks(survival_rate_by_class.index)
plt.show()

# Filter out rows with missing age data
titanic_data = titanic_dataset.dropna(subset=['Age'])

# Separate data into survived and not survived groups
survived = titanic_data[titanic_data['Survived'] == 1]
not_survived = titanic_data[titanic_data['Survived'] == 0]

# Plot age against survival
plt.figure(figsize=(10, 6))
plt.hist([survived['Age'], not_survived['Age']], bins=20, color=['skyblue', 'salmon'], edgecolor='black', label=['Survived', 'Not Survived'])
plt.title('Age Distribution by Survival Status')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.show()

#preprocessing data

#Dropping irrelevant columns
titanic_dataset.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1, inplace = True)
print(titanic_dataset)
print(titanic_dataset.describe())
print(titanic_dataset.info())

#Handling missing values
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')
#imputing Age
titanic_dataset['Age'] = imputer.fit_transform(titanic_dataset[['Age']])
#imputing Embarked
titanic_dataset['Embarked'].fillna('S', inplace=True)
print(titanic_dataset.info())
print(titanic_dataset.head())

X = titanic_dataset.drop('Survived', axis = 1)
y = titanic_dataset['Survived']

print('Matrix of features:\n ', X)
print('Dependent variable:\n ',y)

#Encoding categorical data
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
# Identify the categorical data
categorical_features = ['Sex', 'Embarked', 'Pclass']
# Implement an instance of the ColumnTransformer class
ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), categorical_features)], remainder = 'passthrough')
X = ct.fit_transform(X)
X = pd.DataFrame(X)
print(X.head())

X = np.array(X)
print(X)

print('Encoded Categorical Matrix of features:\n ', X)

#Encoding the dependent variable
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print('Encoded Categorical dependent variable vector:\n ', y)

#Splitting the dataset into train and test
from sklearn.model_selection import StratifiedShuffleSplit
# Initialize StratifiedShuffleSplit
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)
# Split the data using StratifiedShuffleSplit
for train_index, test_index in sss.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
print('x-train: ',X_train)
print('x-test: ',X_test)
print('y-train: ',y_train)
print('y-test: ',y_test)

#Feature Scaling
#standardisation:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
print('Featured Scaled x-train:\n ',X_train)
print('Featured Scaled x-test:\n ',X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Define classifiers
classifiers = {
    "Random Forest Classifier": RandomForestClassifier(n_estimators=100, random_state=42),
    "K-Nearest Neighbors Classifier": KNeighborsClassifier(n_neighbors=5),
    "Support Vector Machine Classifier": SVC(kernel='rbf', random_state=42)
}

accuracies = []

# Loop through classifiers
for name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(f"{name} Accuracy: {accuracy}")

# Plotting the accuracies
fig, ax = plt.subplots()
models = classifiers.keys()
y_pos = np.arange(len(models))
ax.barh(y_pos, accuracies, align='center')
ax.set_yticks(y_pos)
ax.set_yticklabels(models)
ax.invert_yaxis()
ax.set_xlabel('Accuracy')
ax.set_title('Classifier Accuracies')
plt.show()

# Find the best model
best_accuracy = max(accuracies)
best_model = list(classifiers.keys())[accuracies.index(best_accuracy)]
print(f"\nBest Model: {best_model} with Accuracy: {best_accuracy}")

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient
    'kernel': ['rbf']  # Kernel type
}

# Initialize the SVM classifier
svm_classifier = SVC(kernel='rbf')

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')

# Perform grid search
grid_search.fit(X_train, y_train)

# Extract results
results = grid_search.cv_results_
means = results['mean_test_score']
params = results['params']


# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

# Instantiate SVM Classifier with best parameters
best_svm_classifier = SVC(**best_params)

# Train the model with best parameters
best_svm_classifier.fit(X_train, y_train)

# Predictions and accuracy with tuned SVM Classifier
svm_pred_tuned = best_svm_classifier.predict(X_test)
svm_accuracy_tuned = accuracy_score(y_test, svm_pred_tuned)
print("Tuned Support Vector Machine Classifier Accuracy:", svm_accuracy_tuned)

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# Confusion matrix
conf_matrix = confusion_matrix(y_test, svm_pred_tuned)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy score
accuracy = accuracy_score(y_test, svm_pred_tuned)
print("Accuracy Score:", accuracy)

# Precision
precision = precision_score(y_test, svm_pred_tuned, average='weighted')
print("Precision:", precision)

# Recall
recall = recall_score(y_test, svm_pred_tuned, average='weighted')
print("Recall:", recall)

# F1 Score
f1 = f1_score(y_test, svm_pred_tuned, average='weighted')
print("F1 Score:", f1)